EXPERIMENT 11: DENSE NEURAL NETWORK

Parameters
	epochs:			20
	batch_size:		32
	reg:			l2(5e-4)
	activation:		tanh
	class_weight:	{0: 1.1560559993526087, 1: 7.4079561448996225}

Model
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (32, 512)                 46592     
_________________________________________________________________
dropout_1 (Dropout)          (32, 512)                 0         
_________________________________________________________________
dense_2 (Dense)              (32, 512)                 262656    
_________________________________________________________________
dropout_2 (Dropout)          (32, 512)                 0         
_________________________________________________________________
dense_3 (Dense)              (32, 256)                 131328    
_________________________________________________________________
dropout_3 (Dropout)          (32, 256)                 0         
_________________________________________________________________
dense_4 (Dense)              (32, 1)                   257       
=================================================================
Total params: 440,833
Trainable params: 440,833
Non-trainable params: 0
_________________________________________________________________

Data shape
	X_train shape:	(200000, 90)
	y_train shape:	(200000,)
	X_test shape:	(100000, 90)
	y_test shape:	(100000,)

Results on train set
	Loss:		0.016690379921061555
	Accuracy:	0.99415
	Roc_auc:	0.9998246577391048

Results on test set
	Loss_keras:	1.8273002449798583
	Loss:		1.5659845083103043
	Accuracy:	0.62718
	Roc_auc:	0.6549080562500749


Changed MinMaxScaler to StandardScaler to normalize the data, it has done
a pretty good job